<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Document</title>

    	<style>
	body {
		background: #eeeeee;
		font-family: sans-serif;
        display: flex;
        flex-direction: row;
	}

	canvas {
		margin-left:10px;
		margin-top:10px;
		box-shadow: 4px 4px 8px rgba(0,0,0,0.5);
		background: #65AFE8;
        background-image: linear-gradient(#4F88B5, #65AFE8);
	}

	#controls{
		margin-left:10px;
		margin-top:10px;
        border: 2px solid black;
        padding: 5px;
  }

  section{
  	margin-bottom:1em;
  }

  #playButton{
  	font-size: 1.2em;
  	width: 3.5em;
    background-color: pink;
      border: none;
  }

	button[data-playing="yes"]:after{
		content: "Pause";
	}

	button[data-playing="no"]:after{
		content: "Play";
	}

	#fsButton{
  	font-size: 1.2em;
  	width: 6em;
  }
	</style>
	<script>
		"use strict";

		window.onload = init;

		// SCRIPT SCOPED VARIABLES

		// 1- here we are faking an enumeration - we'll look at another way to do this soon
		const SOUND_PATH = Object.freeze({
			sound1: "media/New Adventure Theme.mp3",
			sound2: "media/Peanuts Theme.mp3",
			sound3:  "media/The Picard Song.mp3"
		});

		// 2 - elements on the page
		let audioElement,canvasElement;

		// UI
		let playButton;

		// 3 - our canvas drawing context
		let drawCtx;

		// 4 - our WebAudio context
		let audioCtx;

		// 5 - nodes that are part of our WebAudio audio routing graph
		let sourceNode, analyserNode, gainNode;

		// 6 - a typed array to hold the audio frequency data
		const NUM_SAMPLES = 256;
		// create a new array of 8-bit integers (0-255)
		let audioData = new Uint8Array(NUM_SAMPLES/2);

		let maxRadius = 200;

		//these help with pixel effects
		let invert = true, tintRed = false, noise = true, sepia = true;

        let rooster = new Image();
        rooster.src = 'rooster.jpg';

        let egg = new Image();
        egg.src = 'egg.png';

        let sun = new Image();
        sun.src = 'sun.png';

		// FUNCTIONS
		function init(){
			setupWebaudio();
			setupCanvas();
			setupUI();
			update();

		}

		function setupWebaudio(){
			// 1 - The || is because WebAudio has not been standardized across browsers yet
			const AudioContext = window.AudioContext || window.webkitAudioContext;
			audioCtx = new AudioContext();

			// 2 - get a reference to the <audio> element on the page
			audioElement = document.querySelector("audio");
			audioElement.src = SOUND_PATH.sound3;

			// 3 - create an a source node that points at the <audio> element
			sourceNode = audioCtx.createMediaElementSource(audioElement);

			// 4 - create an analyser node
			analyserNode = audioCtx.createAnalyser();

			/*
			We will request NUM_SAMPLES number of samples or "bins" spaced equally
			across the sound spectrum.

			If NUM_SAMPLES (fftSize) is 256, then the first bin is 0 Hz, the second is 172 Hz,
			the third is 344Hz. Each bin contains a number between 0-255 representing
			the amplitude of that frequency.
			*/

			// fft stands for Fast Fourier Transform
			analyserNode.fftSize = NUM_SAMPLES;

			// 5 - create a gain (volume) node
			gainNode = audioCtx.createGain();
			gainNode.gain.value = 1;

			// 6 - connect the nodes - we now have an audio graph
			sourceNode.connect(analyserNode);
			analyserNode.connect(gainNode);
			gainNode.connect(audioCtx.destination);
		}

		function setupCanvas(){
			canvasElement = document.querySelector('canvas');
			drawCtx = canvasElement.getContext("2d");
		}

		function setupUI(){
			playButton = document.querySelector("#playButton");
			playButton.onclick = e => {
				console.log(`audioCtx.state = ${audioCtx.state}`);

				// check if context is in suspended state (autoplay policy)
				if (audioCtx.state == "suspended") {
					audioCtx.resume();
				}

				if (e.target.dataset.playing == "no") {
					audioElement.play();
					e.target.dataset.playing = "yes";
				// if track is playing pause it
				} else if (e.target.dataset.playing == "yes") {
					audioElement.pause();
					e.target.dataset.playing = "no";
				}

			};

			let volumeSlider = document.querySelector("#volumeSlider");
			volumeSlider.oninput = e => {
				gainNode.gain.value = e.target.value;
				volumeLabel.innerHTML = Math.round((e.target.value/2 * 100));
			};
			volumeSlider.dispatchEvent(new InputEvent("input"));

			let radiusSlider = document.querySelector("#radiusSlider");
			radiusSlider.oninput = e => {
				maxRadius = e.target.value;
				radiusLabel.innerHTML = maxRadius;
			};
			radiusSlider.dispatchEvent(new InputEvent("input"));


			document.querySelector("#trackSelect").onchange = e =>{
				audioElement.src = e.target.value;
				// pause the current track if it is playing
				playButton.dispatchEvent(new MouseEvent("click"));
			};


			// if track ends
			audioElement.onended =  _ => {
				playButton.dataset.playing = "no";
			};

			document.querySelector("#fsButton").onclick = _ =>{
				requestFullscreen(canvasElement);
			};

		}

		function update() {
			// this schedules a call to the update() method in 1/60 seconds
			requestAnimationFrame(update);

			/*
				Nyquist Theorem
				http://whatis.techtarget.com/definition/Nyquist-Theorem
				The array of data we get back is 1/2 the size of the sample rate
			*/

			// populate the audioData with the frequency data
			// notice these arrays are passed "by reference"
			analyserNode.getByteFrequencyData(audioData);

			// OR
			//analyserNode.getByteTimeDomainData(audioData); // waveform data

			// DRAW!
			drawCtx.clearRect(0,0,800,600);

            hills();

            drawCtx.drawImage(rooster, canvasElement.width - 100, canvasElement.height - 100, 100, 100);

            for (let i = 0; i < 10; i++) {
                drawCtx.drawImage(egg, canvasElement.width - (150 + (i * 50)), canvasElement.height - 25, 25, 25);
            };

            drawCtx.drawImage(sun, canvasElement.width - 100, 0, 100, 100);



		}



		// HELPER FUNCTIONS
		function requestFullscreen(element) {
			if (element.requestFullscreen) {
			  element.requestFullscreen();
			} else if (element.mozRequestFullscreen) {
			  element.mozRequestFullscreen();
			} else if (element.mozRequestFullScreen) { // camel-cased 'S' was changed to 's' in spec
			  element.mozRequestFullScreen();
			} else if (element.webkitRequestFullscreen) {
			  element.webkitRequestFullscreen();
			}
			// .. and do nothing if the method is not supported
		};

        function hills() {
            drawCtx.beginPath();
            drawCtx.lineWidth = "3";
            drawCtx.strokeStyle = "green";
            drawCtx.fillStyle = "green";
            drawCtx.moveTo(0, 400);
            drawCtx.lineTo(0, 250);
            drawCtx.bezierCurveTo(0, 250, 200, 250, 300, 450);
            drawCtx.lineTo(0, 450);
            drawCtx.stroke();
            drawCtx.fill();
        }

    </script>
</head>

<body>
    <canvas width="640" height="400"></canvas>
    <div id="controls">
        <audio></audio>
        <section>
            <label>Track:
			<select id="trackSelect">
				<option value="media/New Adventure Theme.mp3">New Adventure Theme</option>
				<option value="media/Peanuts Theme.mp3">Peanuts Theme</option>
				<option value="media/The Picard Song.mp3" selected>The Picard Song</option>
			</select>
		</label>
            <button id="playButton" data-playing="no"></button>
            <button id="fsButton">Full Screen</button>
        </section>
        <section>
            Volume: <input type="range" id="volumeSlider" min="0" max="2" value="1" step="0.01">
            <span id="volumeLabel">???</span>
        </section>

        <section>
            Radius: <input type="range" id="radiusSlider" min="50" max="400" value="1" step="1">
            <span id="radiusLabel">???</span>
        </section>
    </div>
</body>

</html>
