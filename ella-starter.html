<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Document</title>

    <style>
        body {
            background: #eeeeee;
            font-family: sans-serif;
            display: flex;
            flex-direction: row;
        }

        canvas {
            margin-left: 10px;
            margin-top: 10px;
            box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.5);
            background: #65AFE8;
            background-image: linear-gradient(#4F88B5, #65AFE8);
        }

        #controls {
            margin-left: 10px;
            margin-top: 10px;
            border: 2px solid black;
            padding: 5px;
        }

        section {
            margin-bottom: 1em;
        }

        #playButton {
            font-size: 1.2em;
            width: 3.5em;
            background-color: pink;
            border: none;
        }

        button[data-playing="yes"]:after {
            content: "Pause";
        }

        button[data-playing="no"]:after {
            content: "Play";
        }

        #fsButton {
            font-size: 1.2em;
            width: 6em;
        }
    </style>
    <script>
        "use strict";

        window.onload = init;

        // SCRIPT SCOPED VARIABLES

        // 1- here we are faking an enumeration - we'll look at another way to do this soon
        const SOUND_PATH = Object.freeze({
            sound1: "testAudio/bornThisWay.mp3",
            sound2: "testAudio/chickenSong.mp3",
            sound3: "testAudio/sicEmOnAChicken.mp3",
            sound4: "testAudio/chickenFried.mp3"
        });

        // 2 - elements on the page
        let audioElement, canvasElement;

        // UI
        let playButton;

        // 3 - our canvas drawing context
        let drawCtx;

        // 4 - our WebAudio context
        let audioCtx;

        // 5 - nodes that are part of our WebAudio audio routing graph
        let sourceNode, analyserNode, gainNode;

        // 6 - a typed array to hold the audio frequency data
        const NUM_SAMPLES = 256;
        // create a new array of 8-bit integers (0-255)
        let audioData = new Uint8Array(NUM_SAMPLES / 2);

        let maxRadius = 200;

        //these help with pixel effects
        let invert = true,
            tintRed = false,
            noise = true,
            sepia = true;

        let rooster = new Image();
        rooster.src = 'images/martha.png';

        // FUNCTIONS
        function init() {
            setupWebaudio();
            setupCanvas();
            setupUI();
            update();

        }

        function setupWebaudio() {
            // 1 - The || is because WebAudio has not been standardized across browsers yet
            const AudioContext = window.AudioContext || window.webkitAudioContext;
            audioCtx = new AudioContext();

            // 2 - get a reference to the <audio> element on the page
            audioElement = document.querySelector("audio");
            audioElement.src = SOUND_PATH.sound3;

            // 3 - create an a source node that points at the <audio> element
            sourceNode = audioCtx.createMediaElementSource(audioElement);

            // 4 - create an analyser node
            analyserNode = audioCtx.createAnalyser();

            /*
            We will request NUM_SAMPLES number of samples or "bins" spaced equally
            across the sound spectrum.

            If NUM_SAMPLES (fftSize) is 256, then the first bin is 0 Hz, the second is 172 Hz,
            the third is 344Hz. Each bin contains a number between 0-255 representing
            the amplitude of that frequency.
            */

            // fft stands for Fast Fourier Transform
            analyserNode.fftSize = NUM_SAMPLES;

            // 5 - create a gain (volume) node
            gainNode = audioCtx.createGain();
            gainNode.gain.value = 1;

            // 6 - connect the nodes - we now have an audio graph
            sourceNode.connect(analyserNode);
            analyserNode.connect(gainNode);
            gainNode.connect(audioCtx.destination);
        }

        function setupCanvas() {
            canvasElement = document.querySelector('canvas');
            drawCtx = canvasElement.getContext("2d");
        }

        function setupUI() {
            playButton = document.querySelector("#playButton");
            playButton.onclick = e => {
                console.log(`audioCtx.state = ${audioCtx.state}`);

                // check if context is in suspended state (autoplay policy)
                if (audioCtx.state == "suspended") {
                    audioCtx.resume();
                }

                if (e.target.dataset.playing == "no") {
                    audioElement.play();
                    e.target.dataset.playing = "yes";
                    // if track is playing pause it
                } else if (e.target.dataset.playing == "yes") {
                    audioElement.pause();
                    e.target.dataset.playing = "no";
                }

            };

            let volumeSlider = document.querySelector("#volumeSlider");
            volumeSlider.oninput = e => {
                gainNode.gain.value = e.target.value;
                volumeLabel.innerHTML = Math.round((e.target.value / 2 * 100));
            };
            volumeSlider.dispatchEvent(new InputEvent("input"));

            document.querySelector("#trackSelect").onchange = e => {
                audioElement.src = e.target.value;
                // pause the current track if it is playing
                playButton.dispatchEvent(new MouseEvent("click"));
            };


            // if track ends
            audioElement.onended = _ => {
                playButton.dataset.playing = "no";
            };

            document.querySelector("#fsButton").onclick = _ => {
                requestFullscreen(canvasElement);
            };

        }

        function update() {
            // this schedules a call to the update() method in 1/60 seconds
            requestAnimationFrame(update);

            /*
            	Nyquist Theorem
            	http://whatis.techtarget.com/definition/Nyquist-Theorem
            	The array of data we get back is 1/2 the size of the sample rate
            */

            // populate the audioData with the frequency data
            // notice these arrays are passed "by reference"
            analyserNode.getByteFrequencyData(audioData);

            // OR
            //analyserNode.getByteTimeDomainData(audioData); // waveform data

            // DRAW!
            drawCtx.clearRect(0, 0, 800, 600);

            //hills();

            drawCtx.drawImage(rooster, canvasElement.width - 150, canvasElement.height - 150, 150, 150);

            let barWidth = 4;
            let barSpacing = 1;
            let barHeight = 100;
            let topSpacing = 525;

            for (let i = 0; i < audioData.length; i++) {

                //for (let j = 0; j < audioData.length; j++) {
                //grass
                drawCtx.save();
                drawCtx.beginPath();
                drawCtx.strokeStyle = "green";
                drawCtx.moveTo((i * 6), 550);
                //drawCtx.bezierCurveTo(i * 4, 550, (i * 4), 540, (i * 4), (audioData[i] * 0.5) + 500); - straight lines
                drawCtx.bezierCurveTo(i * 6, 550, (i * 6 + 25), 530, (i * 6 + 27), (audioData[i] * .4) + 520);
                drawCtx.stroke();
                drawCtx.closePath();
                drawCtx.restore();
                //}


                //eggs
                drawCtx.beginPath();
                drawCtx.save();
                changeEggColor();
                drawCtx.scale(.75, 1);
                drawCtx.arc(750 - i * (barHeight + barSpacing), topSpacing - audioData[i] * 0.5, barHeight - 75, 0, 2 * Math.PI, false);
                drawCtx.fill();
                drawCtx.stroke();
                drawCtx.closePath();
                drawCtx.restore();


            }

        }

        // HELPER FUNCTIONS

        function requestFullscreen(element) {
            if (element.requestFullscreen) {
                element.requestFullscreen();
            } else if (element.mozRequestFullscreen) {
                element.mozRequestFullscreen();
            } else if (element.mozRequestFullScreen) { // camel-cased 'S' was changed to 's' in spec
                element.mozRequestFullScreen();
            } else if (element.webkitRequestFullscreen) {
                element.webkitRequestFullscreen();
            }
            // .. and do nothing if the method is not supported
        };

        function changeEggColor() {
           // if(document.querySelector(""))
        }
    </script>
</head>

<body>
    <canvas width="750" height="550"></canvas>
    <div id="controls">
                <audio></audio>

                <!-- CHOOSE AUDIO-->
                <section>
                    <label>Track:
                        <select id="trackSelect">
                            <option value="testAudio/bornThisWay.mp3">Born This Way</option>
                            <option value="testAudio/chickenSong.mp3">The Chicken Song</option>
                            <option value="testAudio/sicEmOnAChicken.mp3" selected>Sic 'Em On A Chicken</option>
                            <option value="testAudio/chickenFried.mp3" selected>Chicken Fried</option>
                        </select>
                    </label>

                    <!--PLAY BUTTON-->
                    <button id="playButton" data-playing="no"></button>
                    <!--FULL SCREEN-->
                    <button id="fsButton">Full Screen</button>
                </section>

                <!--SLIDERS-->

                <!--VOLUME SLIDER-->
                <section class="sliders">
                    Volume: <input type="range" id="volumeSlider" min="0" max="2" value="1" step="0.01">
                    <span id="volumeLabel">???</span>
                </section>
                <!--BRIGHTNESS SLIDER-->
                <section class="sliders">
                    Brightness: <input type="range" id="reverbSlider" min="0" max="1" value="0" step="0.01"> <span id="reverbLabel">???</span>
                </section>

                <!--RADIO BUTTONS-->

                <!--GAY-->
                <section class="radio">
                    Gay: <input type="radio" name="pride" value="pan">
                </section>
                <!--LESBIAN-->
                <section class="radio">
                    Lesbian: <input type="radio" name="pride" value="pan">
                </section>
                <!--BI-->
                <section class="radio">
                    Bisexual: <input type="radio" name="pride" value="bi">
                </section>
                <!--PAN-->
                <section class="radio">
                    Pansexual: <input type="radio" name="pride" value="pan">
                </section>
                <!--ACE-->
                <section class="radio">
                    Asexual: <input type="radio" name="pride" value="pan">
                </section>
            </div>
</body>

</html>
